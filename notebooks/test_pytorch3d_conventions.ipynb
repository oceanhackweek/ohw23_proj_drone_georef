{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.renderer import PerspectiveCameras\n",
    "from pytorch3d.transforms import euler_angles_to_matrix\n",
    "from pytorch3d.renderer.utils import format_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PerspectiveCameras in module pytorch3d.renderer.cameras:\n",
      "\n",
      "class PerspectiveCameras(CamerasBase)\n",
      " |  PerspectiveCameras(focal_length: Union[float, Sequence[Tuple[float]], Sequence[Tuple[float, float]], torch.Tensor] = 1.0, principal_point=((0.0, 0.0),), R: torch.Tensor = tensor([[[1., 0., 0.],\n",
      " |           [0., 1., 0.],\n",
      " |           [0., 0., 1.]]]), T: torch.Tensor = tensor([[0., 0., 0.]]), K: Optional[torch.Tensor] = None, device: Union[str, torch.device] = 'cpu', in_ndc: bool = True, image_size: Union[List, Tuple, torch.Tensor, NoneType] = None) -> None\n",
      " |  \n",
      " |  A class which stores a batch of parameters to generate a batch of\n",
      " |  transformation matrices using the multi-view geometry convention for\n",
      " |  perspective camera.\n",
      " |  \n",
      " |  Parameters for this camera are specified in NDC if `in_ndc` is set to True.\n",
      " |  If parameters are specified in screen space, `in_ndc` must be set to False.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PerspectiveCameras\n",
      " |      CamerasBase\n",
      " |      pytorch3d.renderer.utils.TensorProperties\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, focal_length: Union[float, Sequence[Tuple[float]], Sequence[Tuple[float, float]], torch.Tensor] = 1.0, principal_point=((0.0, 0.0),), R: torch.Tensor = tensor([[[1., 0., 0.],\n",
      " |           [0., 1., 0.],\n",
      " |           [0., 0., 1.]]]), T: torch.Tensor = tensor([[0., 0., 0.]]), K: Optional[torch.Tensor] = None, device: Union[str, torch.device] = 'cpu', in_ndc: bool = True, image_size: Union[List, Tuple, torch.Tensor, NoneType] = None) -> None\n",
      " |      Args:\n",
      " |          focal_length: Focal length of the camera in world units.\n",
      " |              A tensor of shape (N, 1) or (N, 2) for\n",
      " |              square and non-square pixels respectively.\n",
      " |          principal_point: xy coordinates of the center of\n",
      " |              the principal point of the camera in pixels.\n",
      " |              A tensor of shape (N, 2).\n",
      " |          in_ndc: True if camera parameters are specified in NDC.\n",
      " |              If camera parameters are in screen space, it must\n",
      " |              be set to False.\n",
      " |          R: Rotation matrix of shape (N, 3, 3)\n",
      " |          T: Translation matrix of shape (N, 3)\n",
      " |          K: (optional) A calibration matrix of shape (N, 4, 4)\n",
      " |              If provided, don't need focal_length, principal_point\n",
      " |          image_size: (height, width) of image size.\n",
      " |              A tensor of shape (N, 2) or a list/tuple. Required for screen cameras.\n",
      " |          device: torch.device or string\n",
      " |  \n",
      " |  get_ndc_camera_transform(self, **kwargs) -> pytorch3d.transforms.transform3d.Transform3d\n",
      " |      Returns the transform from camera projection space (screen or NDC) to NDC space.\n",
      " |      If the camera is defined already in NDC space, the transform is identity.\n",
      " |      For cameras defined in screen space, we adjust the principal point computation\n",
      " |      which is defined in the image space (commonly) and scale the points to NDC space.\n",
      " |      \n",
      " |      This transform leaves the depth unchanged.\n",
      " |      \n",
      " |      Important: This transforms assumes PyTorch3D conventions for the input points,\n",
      " |      i.e. +X left, +Y up.\n",
      " |  \n",
      " |  get_principal_point(self, **kwargs) -> torch.Tensor\n",
      " |      Return the camera's principal point\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: parameters for the camera extrinsics can be passed in\n",
      " |              as keyword arguments to override the default values\n",
      " |              set in __init__.\n",
      " |  \n",
      " |  get_projection_transform(self, **kwargs) -> pytorch3d.transforms.transform3d.Transform3d\n",
      " |      Calculate the projection matrix using the\n",
      " |      multi-view geometry convention.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: parameters for the projection can be passed in as keyword\n",
      " |              arguments to override the default values set in __init__.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `Transform3d` object with a batch of `N` projection transforms.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          fx = focal_length[:, 0]\n",
      " |          fy = focal_length[:, 1]\n",
      " |          px = principal_point[:, 0]\n",
      " |          py = principal_point[:, 1]\n",
      " |      \n",
      " |          K = [\n",
      " |                  [fx,   0,   px,   0],\n",
      " |                  [0,   fy,   py,   0],\n",
      " |                  [0,    0,    0,   1],\n",
      " |                  [0,    0,    1,   0],\n",
      " |          ]\n",
      " |  \n",
      " |  in_ndc(self)\n",
      " |      Specifies whether the camera is defined in NDC space\n",
      " |      or in screen (image) space\n",
      " |  \n",
      " |  is_perspective(self)\n",
      " |  \n",
      " |  unproject_points(self, xy_depth: torch.Tensor, world_coordinates: bool = True, from_ndc: bool = False, **kwargs) -> torch.Tensor\n",
      " |      Args:\n",
      " |          from_ndc: If `False` (default), assumes xy part of input is in\n",
      " |              NDC space if self.in_ndc(), otherwise in screen space. If\n",
      " |              `True`, assumes xy is in NDC space even if the camera\n",
      " |              is defined in screen space.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from CamerasBase:\n",
      " |  \n",
      " |  __getitem__(self, index: Union[int, List[int], torch.BoolTensor, torch.LongTensor]) -> 'CamerasBase'\n",
      " |      Override for the __getitem__ method in TensorProperties which needs to be\n",
      " |      refactored.\n",
      " |      \n",
      " |      Args:\n",
      " |          index: an integer index, list/tensor of integer indices, or tensor of boolean\n",
      " |              indicators used to filter all the fields in the cameras given by self._FIELDS.\n",
      " |      Returns:\n",
      " |          an instance of the current cameras class with only the values at the selected index.\n",
      " |  \n",
      " |  clone(self)\n",
      " |      Returns a copy of `self`.\n",
      " |  \n",
      " |  get_camera_center(self, **kwargs) -> torch.Tensor\n",
      " |      Return the 3D location of the camera optical center\n",
      " |      in the world coordinates.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: parameters for the camera extrinsics can be passed in\n",
      " |              as keyword arguments to override the default values\n",
      " |              set in __init__.\n",
      " |      \n",
      " |      Setting R or T here will update the values set in init as these\n",
      " |      values may be needed later on in the rendering pipeline e.g. for\n",
      " |      lighting calculations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          C: a batch of 3D locations of shape (N, 3) denoting\n",
      " |          the locations of the center of each camera in the batch.\n",
      " |  \n",
      " |  get_full_projection_transform(self, **kwargs) -> pytorch3d.transforms.transform3d.Transform3d\n",
      " |      Return the full world-to-camera transform composing the\n",
      " |      world-to-view and view-to-camera transforms.\n",
      " |      If camera is defined in NDC space, the projected points are in NDC space.\n",
      " |      If camera is defined in screen space, the projected points are in screen space.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: parameters for the projection transforms can be passed in\n",
      " |              as keyword arguments to override the default values\n",
      " |              set in __init__.\n",
      " |      \n",
      " |      Setting R and T here will update the values set in init as these\n",
      " |      values may be needed later on in the rendering pipeline e.g. for\n",
      " |      lighting calculations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          a Transform3d object which represents a batch of transforms\n",
      " |          of shape (N, 3, 3)\n",
      " |  \n",
      " |  get_image_size(self)\n",
      " |      Returns the image size, if provided, expected in the form of (height, width)\n",
      " |      The image size is used for conversion of projected points to screen coordinates.\n",
      " |  \n",
      " |  get_world_to_view_transform(self, **kwargs) -> pytorch3d.transforms.transform3d.Transform3d\n",
      " |      Return the world-to-view transform.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: parameters for the camera extrinsics can be passed in\n",
      " |              as keyword arguments to override the default values\n",
      " |              set in __init__.\n",
      " |      \n",
      " |      Setting R and T here will update the values set in init as these\n",
      " |      values may be needed later on in the rendering pipeline e.g. for\n",
      " |      lighting calculations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A Transform3d object which represents a batch of transforms\n",
      " |          of shape (N, 3, 3)\n",
      " |  \n",
      " |  get_znear(self)\n",
      " |  \n",
      " |  transform_points(self, points, eps: Optional[float] = None, **kwargs) -> torch.Tensor\n",
      " |      Transform input points from world to camera space.\n",
      " |      If camera is defined in NDC space, the projected points are in NDC space.\n",
      " |      If camera is defined in screen space, the projected points are in screen space.\n",
      " |      \n",
      " |      For `CamerasBase.transform_points`, setting `eps > 0`\n",
      " |      stabilizes gradients since it leads to avoiding division\n",
      " |      by excessively low numbers for points close to the camera plane.\n",
      " |      \n",
      " |      Args:\n",
      " |          points: torch tensor of shape (..., 3).\n",
      " |          eps: If eps!=None, the argument is used to clamp the\n",
      " |              divisor in the homogeneous normalization of the points\n",
      " |              transformed to the ndc space. Please see\n",
      " |              `transforms.Transform3d.transform_points` for details.\n",
      " |      \n",
      " |              For `CamerasBase.transform_points`, setting `eps > 0`\n",
      " |              stabilizes gradients since it leads to avoiding division\n",
      " |              by excessively low numbers for points close to the\n",
      " |              camera plane.\n",
      " |      \n",
      " |      Returns\n",
      " |          new_points: transformed points with the same shape as the input.\n",
      " |  \n",
      " |  transform_points_ndc(self, points, eps: Optional[float] = None, **kwargs) -> torch.Tensor\n",
      " |      Transforms points from PyTorch3D world/camera space to NDC space.\n",
      " |      Input points follow the PyTorch3D coordinate system conventions: +X left, +Y up.\n",
      " |      Output points are in NDC space: +X left, +Y up, origin at image center.\n",
      " |      \n",
      " |      Args:\n",
      " |          points: torch tensor of shape (..., 3).\n",
      " |          eps: If eps!=None, the argument is used to clamp the\n",
      " |              divisor in the homogeneous normalization of the points\n",
      " |              transformed to the ndc space. Please see\n",
      " |              `transforms.Transform3d.transform_points` for details.\n",
      " |      \n",
      " |              For `CamerasBase.transform_points`, setting `eps > 0`\n",
      " |              stabilizes gradients since it leads to avoiding division\n",
      " |              by excessively low numbers for points close to the\n",
      " |              camera plane.\n",
      " |      \n",
      " |      Returns\n",
      " |          new_points: transformed points with the same shape as the input.\n",
      " |  \n",
      " |  transform_points_screen(self, points, eps: Optional[float] = None, with_xyflip: bool = True, **kwargs) -> torch.Tensor\n",
      " |      Transforms points from PyTorch3D world/camera space to screen space.\n",
      " |      Input points follow the PyTorch3D coordinate system conventions: +X left, +Y up.\n",
      " |      Output points are in screen space: +X right, +Y down, origin at top left corner.\n",
      " |      \n",
      " |      Args:\n",
      " |          points: torch tensor of shape (..., 3).\n",
      " |          eps: If eps!=None, the argument is used to clamp the\n",
      " |              divisor in the homogeneous normalization of the points\n",
      " |              transformed to the ndc space. Please see\n",
      " |              `transforms.Transform3d.transform_points` for details.\n",
      " |      \n",
      " |              For `CamerasBase.transform_points`, setting `eps > 0`\n",
      " |              stabilizes gradients since it leads to avoiding division\n",
      " |              by excessively low numbers for points close to the\n",
      " |              camera plane.\n",
      " |          with_xyflip: If True, flip x and y directions. In world/camera/ndc coords,\n",
      " |              +x points to the left and +y up. If with_xyflip is true, in screen\n",
      " |              coords +x points right, and +y down, following the usual RGB image\n",
      " |              convention. Warning: do not set to False unless you know what you're\n",
      " |              doing!\n",
      " |      \n",
      " |      Returns\n",
      " |          new_points: transformed points with the same shape as the input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch3d.renderer.utils.TensorProperties:\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  cpu(self) -> 'TensorProperties'\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device: Optional[int] = None) -> 'TensorProperties'\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  gather_props(self, batch_idx) -> 'TensorProperties'\n",
      " |      This is an in place operation to reformat all tensor class attributes\n",
      " |      based on a set of given indices using torch.gather. This is useful when\n",
      " |      attributes which are batched tensors e.g. shape (N, 3) need to be\n",
      " |      multiplied with another tensor which has a different first dimension\n",
      " |      e.g. packed vertices of shape (V, 3).\n",
      " |      \n",
      " |      Example\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          self.specular_color = (N, 3) tensor of specular colors for each mesh\n",
      " |      \n",
      " |      A lighting calculation may use\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          verts_packed = meshes.verts_packed()  # (V, 3)\n",
      " |      \n",
      " |      To multiply these two tensors the batch dimension needs to be the same.\n",
      " |      To achieve this we can do\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          batch_idx = meshes.verts_packed_to_mesh_idx()  # (V)\n",
      " |      \n",
      " |      This gives index of the mesh for each vertex in verts_packed.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          self.gather_props(batch_idx)\n",
      " |          self.specular_color = (V, 3) tensor with the specular color for\n",
      " |                                   each packed vertex.\n",
      " |      \n",
      " |      torch.gather requires the index tensor to have the same shape as the\n",
      " |      input tensor so this method takes care of the reshaping of the index\n",
      " |      tensor to use with class attributes with arbitrary dimensions.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_idx: shape (B, ...) where `...` represents an arbitrary\n",
      " |              number of dimensions\n",
      " |      \n",
      " |      Returns:\n",
      " |          self with all properties reshaped. e.g. a property with shape (N, 3)\n",
      " |          is transformed to shape (B, 3).\n",
      " |  \n",
      " |  isempty(self) -> bool\n",
      " |  \n",
      " |  to(self, device: Union[str, torch.device] = 'cpu') -> 'TensorProperties'\n",
      " |      In place operation to move class properties which are tensors to a\n",
      " |      specified device. If self has a property \"device\", update this as well.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *args, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  forward = _forward_unimplemented(self, *input: Any) -> None\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      \n",
      " |      \n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(module, args) -> None or modified input\n",
      " |      \n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to a module\n",
      " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      " |      respect to module outputs are computed. The hook should have the following\n",
      " |      signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Registers a post hook to be run after module's ``load_state_dict``\n",
      " |      is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      " |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      " |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing references to the whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  call_super_init = False\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(PerspectiveCameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function format_tensor in module pytorch3d.renderer.utils:\n",
      "\n",
      "format_tensor(input, dtype: torch.dtype = torch.float32, device: Union[str, torch.device] = 'cpu') -> torch.Tensor\n",
      "    Helper function for converting a scalar value to a tensor.\n",
      "    \n",
      "    Args:\n",
      "        input: Python scalar, Python list/tuple, torch scalar, 1D torch tensor\n",
      "        dtype: data type for the input\n",
      "        device: Device (as str or torch.device) on which the tensor should be placed.\n",
      "    \n",
      "    Returns:\n",
      "        input_vec: torch tensor with optional added batch dimension.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(format_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function euler_angles_to_matrix in module pytorch3d.transforms.rotation_conversions:\n",
      "\n",
      "euler_angles_to_matrix(euler_angles: torch.Tensor, convention: str) -> torch.Tensor\n",
      "    Convert rotations given as Euler angles in radians to rotation matrices.\n",
      "    \n",
      "    Args:\n",
      "        euler_angles: Euler angles in radians as tensor of shape (..., 3).\n",
      "        convention: Convention string of three uppercase letters from\n",
      "            {\"X\", \"Y\", and \"Z\"}.\n",
      "    \n",
      "    Returns:\n",
      "        Rotation matrices as tensor of shape (..., 3, 3).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(euler_angles_to_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotation_matrix(roll, pitch, yaw):\n",
    "    # Convert angles to radians\n",
    "    # roll = torch.deg2rad(roll)\n",
    "    # pitch = torch.deg2rad(pitch)\n",
    "    # yaw = torch.deg2rad(yaw)\n",
    "\n",
    "    # Calculate the rotation matrix\n",
    "    cos_r = torch.cos(roll)\n",
    "    sin_r = torch.sin(roll)\n",
    "    cos_p = torch.cos(pitch)\n",
    "    sin_p = torch.sin(pitch)\n",
    "    cos_y = torch.cos(yaw)\n",
    "    sin_y = torch.sin(yaw)\n",
    "\n",
    "    R_x = torch.tensor([[1, 0, 0],\n",
    "                        [0, cos_r, -sin_r],\n",
    "                        [0, sin_r, cos_r]])\n",
    "\n",
    "    R_y = torch.tensor([[cos_p, 0, sin_p],\n",
    "                        [0, 1, 0],\n",
    "                        [-sin_p, 0, cos_p]])\n",
    "\n",
    "    R_z = torch.tensor([[cos_y, -sin_y, 0],\n",
    "                        [sin_y, cos_y, 0],\n",
    "                        [0, 0, 1]])\n",
    "\n",
    "    R = R_z @ R_y @ R_x\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7360e+03, 1.8240e+03, 2.5000e-02],\n",
       "        [3.6525e+03, 1.8240e+03, 2.5000e-02],\n",
       "        [1.8195e+03, 1.8240e+03, 2.5000e-02],\n",
       "        [2.7360e+03, 9.0750e+02, 2.5000e-02]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DJI P4RTK Camera Sensor Parameters\n",
    "image_width =  5472\n",
    "image_height = 3648\n",
    "fx, fy = float(3666.), float(3666.) #in pixels\n",
    "\n",
    "px, py = image_width // 2, image_height // 2\n",
    "\n",
    "T = torch.tensor((0.,0.,40.))[None,:]\n",
    "roll = format_tensor(np.deg2rad(0.))\n",
    "pitch = format_tensor(np.deg2rad(0.))\n",
    "yaw = format_tensor(np.deg2rad(0.))\n",
    "R=euler_angles_to_matrix(torch.tensor([roll, pitch, yaw]),convention='ZYX')[None,:]\n",
    "# R2=get_rotation_matrix(roll, pitch, yaw)[None,:]\n",
    "\n",
    "cameras_screen = PerspectiveCameras(focal_length=(fx,),\n",
    "                                    principal_point=((px, py),),\n",
    "                                    image_size=((image_width, image_height),),\n",
    "                                    T=T,\n",
    "                                    R=R,\n",
    "                                    in_ndc=False,\n",
    "                                    device=device)\n",
    "\n",
    "query_point = torch.tensor(((0.,0.,0.),\n",
    "                            (-10.,0.,0.),\n",
    "                            (10.,0,0.),\n",
    "                            (0.,10,0.)))\n",
    "cameras_screen.transform_points_screen(query_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00, -4.3711e-08,  1.0000e+00],\n",
       "          [ 0.0000e+00, -1.0000e+00, -4.3711e-08]]]),\n",
       " tensor([[[-4.3711e-08,  0.0000e+00, -1.0000e+00],\n",
       "          [ 0.0000e+00,  1.0000e+00,  0.0000e+00],\n",
       "          [ 1.0000e+00,  0.0000e+00, -4.3711e-08]]]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R, R2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
